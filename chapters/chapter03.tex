\section{The Cayley Determinant and Aslaksen's Axioms}

In 1845, 2 years after William Rowan Hamilton discovered quaternions, Arthur Cayley attempted to define the determinant of a quaternionic matrix using the usual formula (we denote the Cayley determinant by $Cdet$), i.e., for a $2 \times 2$ quaternionic matrix $A = $ \begin{pmatrix} a & b \\ c & d \end{pmatrix}, $Cdet(A) = ad - cb$ for $a,b,c,d \in \HH$ \cite{aslaksen}. The same goes for $3 \times 3$ matrices and so on. Taking into account the fact that the quaternions are non-commutative (and the implications it has on linear mappings as will be discussed later), we might ask whether or not this determinant behaves the way we expect - Will it really determine whether or not a quaternionic matrix is singular or not? Will the properties of the determinant still hold? Will the determinant still be a map from $M_{n}(G) \rightarrow G$ (in this case, $G = \HH$)? The last question comes from the fact that the determinants of complex matrices is a map from $\Mc{n} \rightarrow \CC$.  


\subsection{The Determinant Function}

We take a step back and revisit what it means for a mapping to be a determinant. We present the determinant function as defined by J.L. Brenner.

\begin{definition}[Determinant Function]
	For a field $F$, a determinant over the matrices of $\Mf{n}$ is a function $det$ from $\Mf{n}$ into $F$ such that 
	\begin{equation}
	det(AB) = det(A)det(B) = det(B)det(A)
	\end{equation} 
	holds either \textbf{(1)} $\forall A, B \in \Mf{n}$ or \textbf{(2)} $\forall$ invertible $A, B \in \Mf{n}$. 
\end{definition}

Notice that the definition requires the images of $A$ and $B$ to commute (this is not possible for skew-fields like the quaternions). We can see this matter viewed in a more rigorous manner (also in a manner more specific to the quaternions) while discussing Aslaksen's axioms. 

We see that if $det$ is a constant function that only maps to either 0 or 1 (with 0 for singular matrices and 1 for invertible matrices), then $det$ satisfies the above definition \cite{brenner}. The following theorem by Brenner shows that this holds for non-trivial determinants as well and that conditions (1) and (2) are essentially equivalent \cite{brenner}. 

\begin{theorem}
If $det$ is not constantly equal to 1 or 0 (i.e., $det$ is not a mapping $det: \Mf{n} \rightarrow F$ where $F$ is a field with two elements), then $det(B) = 0$ for all singular matrices. 
\end{theorem}
\iffalse
\begin{proof}
	Let $O$ be the zero matrix, $I$ the identity matrix, and $A$ a matrix whose determinant (image under $det$) is neither 1 nor 0. 
	Since $OA = O$, by Axiom 2, $det(O)det(A) = det(O)$. Suppose $det(O) \neq 0$, then $det(A) = 1$ which is a contradiction. Hence, $det(O) = 0$.
	Similarly, since $IA = A$, by Axiom 2, $det(I)det(A) = det(A) \implies det(I) = 1$.
	Also note that for a permutation matrix (an elementary matrix that permutes the rows/columns) $E$, $E^m = I$ for some $m$, hence $det(E) \neq 0$.
	Now, define a matrix
	\begin{equation*}
		D = 
		\begin{pmatrix}
			1 &  &  &  &  &  \\
			 & \ddots & & & & \\
			  & & 1 & & & \\
			  & & & 0 & & \\
			  & & & & \ddots & \\
			  & & & & & 0
		\end{pmatrix}
	\end{equation*}
	Notice that if $D$ is singular, then $det(D) = 0$.
\end{proof}
\fi

\subsection{Aslaksen's Axioms}

In the \emph{Mathematical Intelligencer}, Helmer Aslaksen presented 3 determinant \emph{axioms} which a determinant definition must satisfy in order for it to behave the way we expect, i.e., it has the properties we associate with determinants. These axioms were already introduced in Chapter 1 and we will be discussing them in greater detail here. 
\begin{itemize}
	\item \textbf{Axiom 1.} $det(A) = 0$ if and only if $A$ is singular.
	\item \textbf{Axiom 2.} $det(AB) = det(A)det(B)$ for all quaternionic matrices $A$ and $B$.
	\item \textbf{Axiom 3.} If $A'$ is obtained by adding a left-multiple of a row to another row or a right-multiple of a column to another column, then $det(A')=det(A)$.
\end{itemize}



\begin{lemma}

\end{lemma}

\begin{lemma}

\end{lemma}

\begin{theorem}

\end{theorem}

\section{The Study Determinant}

It was not until 1920, that a new approach in defining a quaternionic determinant was presented in a paper by Eduard Study \cite{aslaksen}. His idea involved transforming quaternionic matrices into complex matrices from which one could then just simply take the determinant \cite{aslaksen}. The method involves homomorphisms between quaternionic, complex, and real matrices.

\subsection{Matrix Homomorphisms}

We look into functions that make it possible for us to represent complex numbers and quaternions as matrices. 
\iffalse
\subsubsection{Representing Complex Numbers as Real Matrices}

In abstract algebra, we saw that we can define a bijection from the field of complex numbers to the 2D-plane ($\R^2$) - a mapping $\Theta : \CC \rightarrow \R^2$ where a complex number $a+b\ib$ is mapped to a vector/point $(a,b)$ in the 2D-plane. Therefore, in order to represent complex numbers as real matrices, we have to find a way to view them as linear transformations over $\R^2$. 

Consider the complex function $f(z) = (a+b\ib)z$. We see that the images of $1$ and $\ib$ are $a+b\ib$ and $-b+a\ib$ respectively. Under the function $\Theta$ (in which case $1$ is mapped to $(1,0)$ while $\ib$ is mapped to $(0,1)$), we seek a matrix in $\Mr{2}$ that maps $(1,0)$ to $\Theta(a+b\ib) = (a,b)$ and $(0,1)$ to $\Theta(-b+a\ib) = (-b,a)$. 
\\
\noindent Let this matrix be $F$ = \begin{pmatrix} \alpha & \beta \\ \chi & \delta \end{pmatrix} where $\alpha, \beta, \chi, \text{ and } \delta \in \R$. Then, 
\begin{equation*}
	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		1 \\ 0 
	\end{pmatrix} = 
	\begin{pmatrix} a \\ b \end{pmatrix} \implies
	\begin{pmatrix}
		\alpha \\ \chi
	\end{pmatrix} =
	\begin{pmatrix} a \\ b \end{pmatrix} \implies 
	\alpha = a; \chi = b \text{ and}

	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		0 \\ 1 
	\end{pmatrix} = 
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\begin{pmatrix}
		\beta \\ \delta
	\end{pmatrix} =
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\beta = -b; \delta = a \\
\end{equation*}
\\
\noindent Therefore, $F$ = \begin{pmatrix} \alpha & \beta \\ \chi & \delta \end{pmatrix} = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}. The matrix $F$ can be seen as the matrix representation of the function $f$ which is defined by multiplying a complex number $z$ by $a+b\ib$. We can therefore see the matrix $F$ as the real matrix representation of the complex number $a+b\ib$.
\fi
\subsubsection{Homomorphisms from $\Mc{n}$ to $\Mr{2n}$}
\iffalse
In the previous subsection, we saw that we can represent complex numbers as $2\times 2$ real matrices. We can then define a mapping from $\CC$ to $\Mr{2}$. We can also show that this mapping is a homomorphism.

\begin{theorem}
	Let $\phi : \CC \rightarrow \Mr{2}$ such that $a+b\ib \mapsto$ \begin{pmatrix} a & -b \\ b & a \end{pmatrix}. Then $\phi$ is an injective homomorphism from $\CC$ to $\Mr{2}.
\end{theorem}

%\begin{proof}
%	Let $z_1 = a+b\ib$ and $z_2 = c+d\ib \in \CC$ . \\
%	Then $\phi(z_1z_2) = \phi[(a+b\ib)(c+d\ib)] = \phi[(ac-bd)+(ad+bc)\ib] = $\begin{pmatrix} (ac-bd) & -(ad+bc) \\ (ad+bc) & (ac-bd)$ \end{pmatrix}. \\ \\
%	Now, $\phi(z_1)\phi(z_2) = \phi[(a+b\ib)]\phi[(c+d\ib)] = $\begin{pmatrix} a & -b \\ b & a \end{pmatrix}\begin{pmatrix} c & -d \\ d & c \end{pmatrix}$ = $\begin{pmatrix} (ac-bd) & -(ad+bc) \\ (ad+bc) & (ac-bd)$ \end{pmatrix}. \\
%	Hence, $\phi(z_1z_2) = \phi(z_1)\phi(z_2)$.
%\end{proof}

\begin{remark}
	We will not include the proof for this theorem as this is merely a special case of Theorem \ref{phimorph} (when $n = 1$). 
\end{remark}
\fi

In order to represent complex matrices as real matrices, notice that every complex matrix can be represented as the sum of a real matrix and a purely imaginary matrix, i.e., for an $n\times n$ matrix $Z$, $Z = A + B\ib$ where $A,B \in \Mr{n}$ \cite{aslaksen}. We define a mapping \begin{equation*} \phi(A+B\ib) = \begin{pmatrix} A & -B \\ B & A \end{pmatrix}  \cite{aslaksen}\end{equation*} 

Before we show that this mapping is an injective homomorphism, we first show that the left distributive laws hold for matrices in $\Mc{n}$.

\begin{theorem}\label{distributive}
	For matrices $A,B,C \in \Mc{n}$, $A(B+C) = AB + AC$.
}
\end{theorem}

\begin{proof}
	Let $A = [a_{ij}]$, $B = [b_{ij}]$, $C = [c_{ij}] \in \Mc{n}$. Then $B+C = [b_{ij}+c_{ij}]$ and \begin{equation} 
	\begin{align*} 
	A(B+C) &= [\sum_{k=1}^{n}a_{ik}(b_{kj}+c_{kj})] = [\sum_{k=1}^{n}(a_{ik}b_{kj}+a_{ik}c_{kj})] \\ 
	&= [\sum_{k=1}^{n}a_{ik}b_{kj} + \sum_{k=1}^{n}a_{ik}c_{kj}] = [\sum_{k=1}^{n}a_{ik}b_{kj}] + [\sum_{k=1}^{n}a_{ik}c_{kj}] = AB + AC 
	\end{align*} \end{equation}
\end{proof}

\begin{remark}
	The same method of proof can be used for the right distributive law. Furthermore, this also holds for matrices in $\Mr{n}$ and $\Mh{n}$.
\end{remark}

\begin{theorem} \label{phimorph}
	Let $\phi: \Mc{n} \rightarrow \Mr{2n}$ such that $C+D\ib \mapsto $ \begin{pmatrix} C & -D \\ D & C \end{pmatrix} where $C+D\ib \in \Mc{n}$. Then $\phi$ is an injective homomorphism. 
\end{theorem}

\begin{proof}
	\textbf{\newline1-1:}
	\begin{equation*}
		\phi(A+B\ib) = \phi(C+D\ib)
		&\implies \begin{pmatrix}A & -B \\ B & A \end{pmatrix} = \begin{pmatrix}C & -D \\ D & C \end{pmatrix}
	\end{equation*}
	\begin{align*}
		\implies A = C \text{ and } B = D \text{ by Matrix Equality}
		\implies A+B\ib = C+D\ib
		\implies \phi \text{ is injective.}
	\end{align*}
	\textbf{Homomorphism: \newline}
	Let $A+B\ib$, $C+D\ib \in \Mc{n}$. Then \begin{align*}
		\phi[(A+B\ib)(C+D\ib)] &= \phi[(A+B\ib)C+(A+B\ib)D\ib] \text{ by Theorem \ref{distributive}} \\
		&= \phi[AC+BC\ib+AD\ib-BD] = \phi[(AC-BD)+(BC+AD)\ib] \\
		&= \begin{pmatrix} (AC-BD) & -(BC+AD) \\ (BC+AD) & (AC-BD) \end{pmatrix}
	\end{align*}
		$\phi[(A+B\ib)]\phi[(C+D\ib)]$ = \begin{pmatrix}A & -B \\ B & A \end{pmatrix}\begin{pmatrix}C & -D \\ D & C \end{pmatrix} \newline \\ = \begin{pmatrix} \genmat{a} & \genmat{-b} \\ \genmat{b} & \genmat{a} \end{pmatrix}\begin{pmatrix} \genmat{c} & \genmat{-d} \\ \genmat{d} & \genmat{c} \end{pmatrix} \\  \\
	= \begin{pmatrix} 
	\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum_{k=1}^{n}b_{1k}d_{k1}}{-\sum_{k=1}^{n}a_{1k}d_{kn} - \sum_{k=1}^{n}b_{1k}c_{kn}}{\sum_{k=1}^{n}b_{nk}c_{k1} + \sum_{k=1}^{n}a_{nk}d_{k1}}{-\sum_{k=1}^{n}b_{nk}d_{kn} + \sum_{k=1}^{n}a_{nk}c_{kn}} 
	%\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum{k=1}^{n}b_{1k}d_{k1}}{\sum_{k=1}^{n}a_{1k}c_{kn} - \sum{k=1}^{n}b_{1k}d_{kn}}{\sum_{k=1}^{n}a_{nk}c_{k1} - \sum{k=1}^{n}b_{nk}d_{k1}}{\sum_{k=1}^{n}a_{nk}c_{kn} - \sum{k=1}^{n}b_{nk}d_{kn}} &
	%\genmatk{-(\sum_{k=1}^{n}a_{1k}d_{k1} + \sum_{k=1}^{n}b_{1k}c{k1})}{-(\sum_{k=1}^{n}a_{1k}d_{kn} + \sum_{k=1}^{n}b_{1k}c_{kn})}{-(\sum_{k=1}^{n}a_{nk}d_{k1} + \sum_{k=1}^{n}b_{nk}c{k1})}{-(\sum_{k=1}^{n}a_{nk}d_{kn} + \sum_{k=1}^{n}b_{nk}c_{kn})} \\
	%\genmatk{\sum_{k=1}^{n}a_{1k}d_{k1} + \sum_{k=1}^{n}b_{1k}c{k1}}{\sum_{k=1}^{n}a_{1k}d_{kn} + \sum_{k=1}^{n}b_{1k}c_{kn}}{\sum_{k=1}^{n}a_{nk}d_{k1} + \sum_{k=1}^{n}b_{nk}c{k1}}{\sum_{k=1}^{n}a_{nk}d_{kn} + \sum_{k=1}^{n}b_{nk}c_{kn}} &
	%\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum{k=1}^{n}b_{1k}d_{k1}}{\sum_{k=1}^{n}a_{1k}c_{kn} - \sum{k=1}^{n}b_{1k}d_{kn}}{\sum_{k=1}^{n}a_{nk}c_{k1} - \sum{k=1}^{n}b_{nk}d_{k1}}{\sum_{k=1}^{n}a_{nk}c_{kn} - \sum{k=1}^{n}b_{nk}d_{kn}}
	\end{pmatrix} \\ \\
	= \begin{pmatrix} (AC-BD) & -(BC+AD) \\ (BC+AD) & (AC-BD) \end{pmatrix}
\end{proof}
\newline
\newline
\begin{definition}[Complex Structure]
	A \emph{complex structure} of a vector space $V$ is defined by the linear map (linear transformation) $J: V \rightarrow V$ such that $J^2 = -I$, where $I$ is the identity map. \cite{wolfram} 
\end{definition}

Complex structures are, in general, linear maps that exhibit the property of the imaginary number $i$, that is, $i^2 = -1$. It is important to note that a linear map \emph{must} commute with scalar multiplication, and thus, representing complex linear maps as real linear maps requires the latter to commute with a complex structure of its vector space (this applies to any associated linear maps between different vector spaces) \cite{aslaksen} \cite{stack}.

We define a matrix \begin{equation*} J = \begin{pmatrix} 0 & -I_n \\ I_n & 0 \end{pmatrix} \end{equation*}. Notice that the matrix $J$ is the image of $iI \in \Mc{n}$ under $\phi$ \cite{aslaksen}. It can be easily shown that $J^2 = -I$. It is obvious that J gives a \emph{complex structure} in $\R^{2n}$. Hence, $\phi(\Mc{n}) = \{P \in \Mr{2n} | JP = PJ\}$, i.e., the real matrix representations of complex matrices are the linear maps in $\Mr{2n}$ that commute with the complex structure \cite{aslaksen}. 

%We will extend the definition of $\phi$ to hold for complex matrices in general. To do this, notice that every complex matrix can be represented as the sum of a real matrix and a purely imaginary matrix, i.e., for an $n\times n$ matrix $N$, $N = C + D\ib$ where $C,D \in \Mr{n}$ \cite{aslaksen}. We see that we can intuitively extend the definition of $\phi$ by defining \begin{equation*} \phi(C+D\ib) = \begin{pmatrix} C & -D \\ D & C \end{pmatrix}  \cite{aslaksen}\end{equation*} 


\iffalse
\subsection{Representing Quaternions as Complex Matrices}

A quaternion $q = a + b\ib + c\jb + d\kb$ can be written as $q = (a+b\ib) + (c+d\ib)\jb = (a+b\ib)+\jb(c-d\ib) = (a+b\ib)-\jb(-c+d\ib)$ where $a+b\ib,c+d\ib,c-d\ib, -c+d\ib \in \CC$. Note that from the latter, we can easily deduce that $v\jb = \jb\bar{v}$ for $v \in \CC$. 

We can therefore view the set of quaternions as a two-dimensional algebra over $\CC$ \cite{stamaria}, i.e., we can define a mapping $\Omega : \HH \rightarrow \CC^2$ such that $\quat{a}{b}{c}{d} \mapsto (a+b\ib,-c+d\ib)$. We can easily show that $\Omega$ is a bijection.

Let us consider the quaternionic function $g(q) = (\quat{a}{b}{c}{d})q$. 


\newcommand{\kmat}{\begin{pmatrix} \kappa & \lambda \\ \mu & \nu \end{pmatrix}}

\newcommand{\vectC}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}

Let this matrix be $G = $ \kmat, where $\kappa, \lambda, \mu, $ and $\nu \in \CC$. Then, 

\kmat \vectC{1}{0} $=$ \vectC{\kappa}{\mu} $=$ \vectC{a+b\ib}{c+d\ib} $\implies$ $\kappa = a+b\ib$ and $\mu = c+d\ib$.

\kmat \vectC{\ib}{0} $=$ \vectC{\kappa \ib}{\mu \ib} $=$ \vectC{-b+a\ib}{d-c\ib} $\implies$ $\kappa = a+\ib$ and $\mu = $
\fi
\subsubsection{Homomorphisms from $\Mh{n}$ to $\Mc{2n}$}

To represent quaternionic matrices as complex matrices, notice that every quaternionic matrix can be represented as the sum $Y = C + \jb D$ where $C, D \in \Mc{n}$ \cite{aslaksen}. We define a mapping \begin{equation*} \psi(C+\jb D) = \begin{pmatrix} C & -\overline{D} \\ D & \overline{C} \end{pmatrix}  \cite{aslaksen}\end{equation*} 

We can show that $\psi$ is an injective homomorphism using the same proof outline in the previous subsection \cite{aslaksen}.

The non-commutativity of quaternions presents some problems in representing \emph{quaternionic linear maps} as complex linear maps. If we consider a quaternionic linear map say $L(v) = Av$ for $A \in \Mh{n}$ where we take in quaternions as scalars, then, $cAv = c L(v) = L(cv) = Acv$ which is false (considering the base case for $1\times 1$ matrices) \cite{stack}. However, $Avc = L(v)c = L(vc) = Avc$. Hence, we now see that any quaternionic linear map commutes with right scalar multiplication by a quaternion which itself is not a linear map in $\HH$ (in order for it to be a linear map, it in turn, has to commute with other quaternions)\cite{stack} \cite{aslaksen}. This poses a problem because it implies that there is no matrix representation for right scalar multiplication \cite{aslaksen}. However, in \cite{aslaksen}, we see that we can consider a linear map $\widetilde{R_j}$ in $\CC^{2n}$ as the image of right scalar multiplication by $\jb$ under the homomorphism. $\widetilde{R_j}$ corresponds to multiplying $v \in \CC^{2n}$ by the matrix $J$ and then conjugating \cite{aslaksen}. This gives a quaternionic structure in $\CC^{2n}$ and thus, a quaternionic linear map corresponds to a complex linear map $Q$ that commutes with $\widetilde{R_j}$, i.e., $Q \overline{Jv} = \overline{JQv}$ for $v \in \CC^{2n}$. It can be easily shown that the latter holds if and only if $\overline{Q}J = JQ$ using the fact that $Q\overline{Jv} = \overline{QJv}$. Thus, $\psi(\Mh{n}) = \{Q \in \Mc{2n} |\overline{Q}J = JQ\}.

\subsection{Study Determinant}

\begin{definition}
	The Study Determinant is defined by $Sdet M = \cdet{\psi{M}} = \sqrt{\rdet{\phi(\psi(M))}}.
\end{definition}

It can be shown that the Study Determinant satisfies all of Aslaksen's axioms \cite{aslaksen}.

\section{Main Result}
\begin{prop}
 $\mathscr{D}_n(\HH)$ is empty when $n$ is odd.
\end{prop}
\begin{proof}
	If $F \in \mathscr{D}_{n}(\HH)$ then $F\bar{F} = -I_n$. \newline Taking the Study determinant of both sides, 
	\begin{align*}
		Sdet(E\bar{E}) &= Sdet(-I_n) \\
		Sdet(E)Sdet(\bar{E}) &= (-1)^n \\
		Sdet(E)\overline{Sdet(E)} &= (-1)^n \text{, by Theorem \ref{detbar}} \\
		|Sdet(E)|^2 &= (-1)^n
	\end{align*}
	Since $|Sdet(E)|^2 > 0$, $(-1)^n > 0$. Hence, $n$ must be even.
\end{proof}

\begin{remark}
	Theorem \ref{detbar} holds because the Study determinant is a complex determinant.
\end{remark}