\section{The Cayley Determinant and Aslaksen's \newline Axioms}

In 1845, 2 years after William Rowan Hamilton discovered quaternions, Arthur Cayley attempted to define the determinant of a quaternionic matrix using the usual formula (we denote the Cayley determinant by $Cdet$), i.e., for a $2 \times 2$ quaternionic matrix $A = $ \begin{pmatrix} a & b \\ c & d \end{pmatrix}, $Cdet(A) = ad - cb$ for $a,b,c,d \in \HH$ \cite{aslaksen}. The same goes for $3 \times 3$ matrices and so on. Taking into account the fact that the quaternions are non-commutative (and the implications it has on linear mappings as discussed in the previous chapter), we might ask whether or not this determinant behaves the way we expect - Will it really determine whether or not a quaternionic matrix is singular or not? Will the properties of the determinant still hold? Will the determinant still be a map from $M_{n}(F) \rightarrow F$ (in this case, $F = \HH$)? The last question comes from the fact that the determinants of complex matrices is a map from $\Mc{n} \rightarrow \C$.  

In the \emph{Mathematical Intelligencer}, Helmer Aslaksen presented 3 determinant \emph{axioms} which a determinant definition must satisfy in order for it to behave the way we expect, i.e., it has the properties we associate with determinants. These axioms were already introduced in Chapter 1. 

The Cayley Determinant doesn't satisfy any of the axioms \cite{aslaksen}. More importantly, it can be shown that the value of a quaternionic determinant must be a commutative subset of $\HH$ \cite{aslaksen} therefore, a quaternionic determinant cannot be a map from $M_{n}(\HH) \rightarrow \HH$. 

\section{The Study Determinant}

It was not until 1920, that a new approach in defining a quaternionic determinant was presented in a paper by Eduard Study \cite{aslaksen}. His idea involved transforming quaternionic matrices into complex matrices from which one could then just simply take the determinant \cite{aslaksen}. The method involves homomorphisms between quaternionic, complex, and real matrices.

\subsection{Matrix Homomorphisms}

We look into functions that make it possible for us to represent complex numbers and quaternions as matrices. 
\iffalse
\subsubsection{Representing Complex Numbers as Real Matrices}

In abstract algebra, we saw that we can define a bijection from the field of complex numbers to the 2D-plane ($\R^2$) - a mapping $\Theta : \C \rightarrow \R^2$ where a complex number $a+b\ib$ is mapped to a vector/point $(a,b)$ in the 2D-plane. Therefore, in order to represent complex numbers as real matrices, we have to find a way to view them as linear transformations over $\R^2$. 

Consider the complex function $f(z) = (a+b\ib)z$. We see that the images of $1$ and $\ib$ are $a+b\ib$ and $-b+a\ib$ respectively. Under the function $\Theta$ (in which case $1$ is mapped to $(1,0)$ while $\ib$ is mapped to $(0,1)$), we seek a matrix in $\Mr{2}$ that maps $(1,0)$ to $\Theta(a+b\ib) = (a,b)$ and $(0,1)$ to $\Theta(-b+a\ib) = (-b,a)$. 
\\
\noindent Let this matrix be $F$ = \begin{pmatrix} \alpha & \beta \\ \chi & \delta \end{pmatrix} where $\alpha, \beta, \chi, \text{ and } \delta \in \R$. Then, 
\begin{equation*}
	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		1 \\ 0 
	\end{pmatrix} = 
	\begin{pmatrix} a \\ b \end{pmatrix} \implies
	\begin{pmatrix}
		\alpha \\ \chi
	\end{pmatrix} =
	\begin{pmatrix} a \\ b \end{pmatrix} \implies 
	\alpha = a; \chi = b \text{ and}

	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		0 \\ 1 
	\end{pmatrix} = 
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\begin{pmatrix}
		\beta \\ \delta
	\end{pmatrix} =
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\beta = -b; \delta = a \\
\end{equation*}
\\
\noindent Therefore, $F$ = \begin{pmatrix} \alpha & \beta \\ \chi & \delta \end{pmatrix} = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}. The matrix $F$ can be seen as the matrix representation of the function $f$ which is defined by multiplying a complex number $z$ by $a+b\ib$. We can therefore see the matrix $F$ as the real matrix representation of the complex number $a+b\ib$.
\fi
\subsubsection{Homomorphisms from $\Mc{n}$ to $\Mr{2n}$}
\iffalse
In the previous subsection, we saw that we can represent complex numbers as $2\times 2$ real matrices. We can then define a mapping from $\C$ to $\Mr{2}$. We can also show that this mapping is a homomorphism.

\begin{theorem}
	Let $\phi : \C \rightarrow \Mr{2}$ such that $a+b\ib \mapsto$ \begin{pmatrix} a & -b \\ b & a \end{pmatrix}. Then $\phi$ is an injective homomorphism from $\C$ to $\Mr{2}.
\end{theorem}

%\begin{proof}
%	Let $z_1 = a+b\ib$ and $z_2 = c+d\ib \in \C$ . \\
%	Then $\phi(z_1z_2) = \phi[(a+b\ib)(c+d\ib)] = \phi[(ac-bd)+(ad+bc)\ib] = $\begin{pmatrix} (ac-bd) & -(ad+bc) \\ (ad+bc) & (ac-bd)$ \end{pmatrix}. \\ \\
%	Now, $\phi(z_1)\phi(z_2) = \phi[(a+b\ib)]\phi[(c+d\ib)] = $\begin{pmatrix} a & -b \\ b & a \end{pmatrix}\begin{pmatrix} c & -d \\ d & c \end{pmatrix}$ = $\begin{pmatrix} (ac-bd) & -(ad+bc) \\ (ad+bc) & (ac-bd)$ \end{pmatrix}. \\
%	Hence, $\phi(z_1z_2) = \phi(z_1)\phi(z_2)$.
%\end{proof}

\begin{remark}
	We will not include the proof for this theorem as this is merely a special case of Theorem \ref{phimorph} (when $n = 1$). 
\end{remark}
\fi

In order to represent complex matrices as real matrices, notice that every complex matrix can be represented as the sum of a real matrix and a purely imaginary matrix, i.e., for an $n\times n$ matrix $Z$, $Z = A + B\ib$ where $A,B \in \Mr{n}$ \cite{aslaksen}. We define a mapping \begin{equation*} \phi(A+B\ib) = \begin{pmatrix} A & -B \\ B & A \end{pmatrix}  \cite{aslaksen}\end{equation*} 

Before we show that this mapping is an injective homomorphism, we first show that the left distributive laws hold for matrices in $\Mc{n}$.

\begin{theorem}\label{distributive}
	For matrices $A,B,C \in \Mc{n}$, $A(B+C) = AB + AC$.
}
\end{theorem}

\begin{proof}
	Let $A = [a_{ij}]$, $B = [b_{ij}]$, $C = [c_{ij}] \in \Mc{n}$. Then $B+C = [b_{ij}+c_{ij}]$ and \begin{equation} 
	\begin{align*} 
	A(B+C) &= [\sum_{k=1}^{n}a_{ik}(b_{kj}+c_{kj})] = [\sum_{k=1}^{n}(a_{ik}b_{kj}+a_{ik}c_{kj})] \\ 
	&= [\sum_{k=1}^{n}a_{ik}b_{kj} + \sum_{k=1}^{n}a_{ik}c_{kj}] = [\sum_{k=1}^{n}a_{ik}b_{kj}] + [\sum_{k=1}^{n}a_{ik}c_{kj}] = AB + AC 
	\end{align*} \end{equation}
\end{proof}

\begin{remark}
	The same method of proof can be used for the right distributive law. Furthermore, this also holds for matrices in $\Mr{n}$ and $\Mh{n}$.
\end{remark}

\begin{theorem} \label{phimorph}
	Let $\phi: \Mc{n} \rightarrow \Mr{2n}$ such that $C+D\ib \mapsto $ \begin{pmatrix} C & -D \\ D & C \end{pmatrix} where $C+D\ib \in \Mc{n}$. Then $\phi$ is an injective homomorphism. 
\end{theorem}

\begin{proof}
	\textbf{\newline1-1:}
	\begin{equation*}
		\phi(A+B\ib) = \phi(C+D\ib)
		&\implies \begin{pmatrix}A & -B \\ B & A \end{pmatrix} = \begin{pmatrix}C & -D \\ D & C \end{pmatrix}
	\end{equation*}
	\begin{align*}
		\implies A = C \text{ and } B = D \text{ by Matrix Equality}
		\implies A+B\ib = C+D\ib
		\implies \phi \text{ is injective.}
	\end{align*}
	\textbf{Homomorphism: \newline}
	Let $A+B\ib$, $C+D\ib \in \Mc{n}$. Then \begin{align*}
		\phi[(A+B\ib)(C+D\ib)] &= \phi[(A+B\ib)C+(A+B\ib)D\ib] \text{ by Theorem \ref{distributive}} \\
		&= \phi[AC+BC\ib+AD\ib-BD] = \phi[(AC-BD)+(BC+AD)\ib] \\
		&= \begin{pmatrix} (AC-BD) & -(BC+AD) \\ (BC+AD) & (AC-BD) \end{pmatrix}
	\end{align*}
		$\phi[(A+B\ib)]\phi[(C+D\ib)]$ = \begin{pmatrix}A & -B \\ B & A \end{pmatrix}\begin{pmatrix}C & -D \\ D & C \end{pmatrix} \newline \\ = \begin{pmatrix} \genmat{a} & \genmat{-b} \\ \genmat{b} & \genmat{a} \end{pmatrix}\begin{pmatrix} \genmat{c} & \genmat{-d} \\ \genmat{d} & \genmat{c} \end{pmatrix} \\  \\
	= \begin{pmatrix} 
	\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum_{k=1}^{n}b_{1k}d_{k1}}{-\sum_{k=1}^{n}a_{1k}d_{kn} - \sum_{k=1}^{n}b_{1k}c_{kn}}{\sum_{k=1}^{n}b_{nk}c_{k1} + \sum_{k=1}^{n}a_{nk}d_{k1}}{-\sum_{k=1}^{n}b_{nk}d_{kn} + \sum_{k=1}^{n}a_{nk}c_{kn}} 
	%\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum{k=1}^{n}b_{1k}d_{k1}}{\sum_{k=1}^{n}a_{1k}c_{kn} - \sum{k=1}^{n}b_{1k}d_{kn}}{\sum_{k=1}^{n}a_{nk}c_{k1} - \sum{k=1}^{n}b_{nk}d_{k1}}{\sum_{k=1}^{n}a_{nk}c_{kn} - \sum{k=1}^{n}b_{nk}d_{kn}} &
	%\genmatk{-(\sum_{k=1}^{n}a_{1k}d_{k1} + \sum_{k=1}^{n}b_{1k}c{k1})}{-(\sum_{k=1}^{n}a_{1k}d_{kn} + \sum_{k=1}^{n}b_{1k}c_{kn})}{-(\sum_{k=1}^{n}a_{nk}d_{k1} + \sum_{k=1}^{n}b_{nk}c{k1})}{-(\sum_{k=1}^{n}a_{nk}d_{kn} + \sum_{k=1}^{n}b_{nk}c_{kn})} \\
	%\genmatk{\sum_{k=1}^{n}a_{1k}d_{k1} + \sum_{k=1}^{n}b_{1k}c{k1}}{\sum_{k=1}^{n}a_{1k}d_{kn} + \sum_{k=1}^{n}b_{1k}c_{kn}}{\sum_{k=1}^{n}a_{nk}d_{k1} + \sum_{k=1}^{n}b_{nk}c{k1}}{\sum_{k=1}^{n}a_{nk}d_{kn} + \sum_{k=1}^{n}b_{nk}c_{kn}} &
	%\genmatk{\sum_{k=1}^{n}a_{1k}c_{k1} - \sum{k=1}^{n}b_{1k}d_{k1}}{\sum_{k=1}^{n}a_{1k}c_{kn} - \sum{k=1}^{n}b_{1k}d_{kn}}{\sum_{k=1}^{n}a_{nk}c_{k1} - \sum{k=1}^{n}b_{nk}d_{k1}}{\sum_{k=1}^{n}a_{nk}c_{kn} - \sum{k=1}^{n}b_{nk}d_{kn}}
	\end{pmatrix} \\ \\
	= \begin{pmatrix} (AC-BD) & -(BC+AD) \\ (BC+AD) & (AC-BD) \end{pmatrix}
\end{proof}
\newline
\newline
\begin{definition}[Complex Structure]
	A \emph{complex structure} of a vector space $V$ is defined by the linear map (linear transformation) $J: V \rightarrow V$ such that $J^2 = -I$, where $I$ is the identity map. \cite{wolfram} 
\end{definition}

Complex structures are, in general, linear maps that exhibit the property of the imaginary number $i$, that is, $i^2 = -1$. It is important to note that a linear map \emph{must} commute with scalar multiplication, and thus, representing complex linear maps as real linear maps requires the latter to commute with a complex structure of its vector space (this applies to any associated linear maps between different vector spaces) \cite{aslaksen} \cite{stack}.

We define a matrix \begin{equation*} J = \begin{pmatrix} 0 & -I_n \\ I_n & 0 \end{pmatrix} \end{equation*}. Notice that the matrix $J$ is the image of $iI \in \Mc{n}$ under $\phi$ \cite{aslaksen}. It can be easily shown that $J^2 = -I$. It is obvious that J gives a \emph{complex structure} in $\R^{2n}$. Hence, $\phi(\Mc{n}) = \{P \in \Mr{2n} | JP = PJ\}$, i.e., the real matrix representations of complex matrices are the linear maps in $\Mr{2n}$ that commute with the complex structure \cite{aslaksen}. 

%We will extend the definition of $\phi$ to hold for complex matrices in general. To do this, notice that every complex matrix can be represented as the sum of a real matrix and a purely imaginary matrix, i.e., for an $n\times n$ matrix $N$, $N = C + D\ib$ where $C,D \in \Mr{n}$ \cite{aslaksen}. We see that we can intuitively extend the definition of $\phi$ by defining \begin{equation*} \phi(C+D\ib) = \begin{pmatrix} C & -D \\ D & C \end{pmatrix}  \cite{aslaksen}\end{equation*} 


\iffalse
\subsection{Representing Quaternions as Complex Matrices}

A quaternion $q = a + b\ib + c\jb + d\kb$ can be written as $q = (a+b\ib) + (c+d\ib)\jb = (a+b\ib)+\jb(c-d\ib) = (a+b\ib)-\jb(-c+d\ib)$ where $a+b\ib,c+d\ib,c-d\ib, -c+d\ib \in \C$. Note that from the latter, we can easily deduce that $v\jb = \jb\bar{v}$ for $v \in \C$. 

We can therefore view the set of quaternions as a two-dimensional algebra over $\C$ \cite{stamaria}, i.e., we can define a mapping $\Omega : \HH \rightarrow \C^2$ such that $\quat{a}{b}{c}{d} \mapsto (a+b\ib,-c+d\ib)$. We can easily show that $\Omega$ is a bijection.

Let us consider the quaternionic function $g(q) = (\quat{a}{b}{c}{d})q$. 


\newcommand{\kmat}{\begin{pmatrix} \kappa & \lambda \\ \mu & \nu \end{pmatrix}}

\newcommand{\vectC}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}

Let this matrix be $G = $ \kmat, where $\kappa, \lambda, \mu, $ and $\nu \in \C$. Then, 

\kmat \vectC{1}{0} $=$ \vectC{\kappa}{\mu} $=$ \vectC{a+b\ib}{c+d\ib} $\implies$ $\kappa = a+b\ib$ and $\mu = c+d\ib$.

\kmat \vectC{\ib}{0} $=$ \vectC{\kappa \ib}{\mu \ib} $=$ \vectC{-b+a\ib}{d-c\ib} $\implies$ $\kappa = a+\ib$ and $\mu = $
\fi
\subsubsection{Homomorphisms from $\Mh{n}$ to $\Mc{2n}$}

To represent quaternionic matrices as complex matrices, notice that every quaternionic matrix can be represented as the sum $Y = C + \jb D$ where $C, D \in \Mc{n}$ \cite{aslaksen}. We define a mapping \begin{equation*} \psi(C+\jb D) = \begin{pmatrix} C & -\overline{D} \\ D & \overline{C} \end{pmatrix}  \cite{aslaksen}\end{equation*} 

We can show that $\psi$ is an injective homomorphism using the same proof outline in the previous subsection \cite{aslaksen}.

The non-commutativity of quaternions presents some problems in representing \emph{quaternionic linear maps} as complex linear maps. If we consider a quaternionic linear map say $L(v) = Av$ for $A \in \Mh{n}$ where we take in quaternions as scalars, then, $cAv = c L(v) = L(cv) = Acv$ which is false (considering the base case for $1\times 1$ matrices) \cite{stack}. However, $Avc = L(v)c = L(vc) = Avc$. Hence, we now see that any quaternionic linear map commutes with right scalar multiplication by a quaternion which itself is not a linear map in $\HH$ (in order for it to be a linear map, it in turn, has to commute with other quaternions)\cite{stack} \cite{aslaksen}. This poses a problem because it implies that there is no matrix representation for right scalar multiplication \cite{aslaksen}. However, in \cite{aslaksen}, we see that we can consider a linear map $\widetilde{R_j}$ in $\C^{2n}$ as the image of right scalar multiplication by $\jb$ under the homomorphism. $\widetilde{R_j}$ corresponds to multiplying $v \in \C^{2n}$ by the matrix $J$ and then conjugating \cite{aslaksen}. This gives a quaternionic structure in $\C^{2n}$ and thus, a quaternionic linear map corresponds to a complex linear map $Q$ that commutes with $\widetilde{R_j}$, i.e., $Q \overline{Jv} = \overline{JQv}$ for $v \in \C^{2n}$. It can be easily shown that the latter holds if and only if $\overline{Q}J = JQ$ using the fact that $Q\overline{Jv} = \overline{QJv}$. Thus, $\psi(\Mh{n}) = \{Q \in \Mc{2n} |\overline{Q}J = JQ\}.

\subsection{Study Determinant}

\begin{definition}
	The Study Determinant is defined by $Sdet M = \cdet{\psi{M}} = \sqrt{\rdet{\phi(\psi(M))}}.
\end{definition}

It can be shown that the Study Determinant satisfies all of Aslaksen's axioms \cite{aslaksen}.

\section{Main Result}
\begin{prop}
 $\mathscr{D}_n(\HH)$ is empty when $n$ is odd.
\end{prop}
\begin{proof}
	If $F \in \mathscr{D}_{n}(\HH)$ then $F\bar{F} = -I_n$. \newline Taking the Study determinant of both sides, 
	\begin{align*}
		Sdet(E\bar{E}) &= Sdet(-I_n) \\
		Sdet(E)Sdet(\bar{E}) &= (-1)^n \\
		Sdet(E)\overline{Sdet(E)} &= (-1)^n \text{, by Theorem \ref{detbar}} \\
		|Sdet(E)|^2 &= (-1)^n
	\end{align*}
	Since $|Sdet(E)|^2 > 0$, $(-1)^n > 0$. Hence, $n$ must be even.
\end{proof}
\begin{remark}
	Theorem \ref{detbar} holds because the Study determinant is a complex determinant.
\end{remark}