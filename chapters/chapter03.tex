In this Chapter, we discuss the implications of quaternionic matrices on defining \emph{quaternionic determinants}. We then look into one Definition of a quaternionic determinant - the \emph{Study Determinant} - which uses matrix homomorphisms that allow us to represent complex matrices as real matrices and quaternionic matrices as complex matrices. 

\section{The Cayley Determinant and Aslaksen's Axioms} \label{cayley}

In 1845, 2 years after William Rowan Hamilton discovered quaternions, Arthur Cayley attempted to define the determinant of a quaternionic matrix using the usual formula (we denote the Cayley determinant by $Cdet$). Note that the following definition can be extended for $n\times n$ quaternionic matrices.

\begin{definition}[$2\times 2$ Cayley Determinant]
 \emph{\cite{aslaksen}} For a $2 \times 2$ quaternionic matrix \newline $A =  \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, $Cdet(A) = ad - cb$ for $a,b,c,d \in \HH$. 
\end{definition}
 
 Notice that the order in which the entries are multiplied matters. We can see that for a $3\times 3$ quaternionic matrix 

 $B = 
\begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix}$, $Cdet(B) = (aei+bfg+cdh)-(gec+hfa+idb)$.
 
 \begin{ex} \label{singbutnot}
 	\cite{aslaksen} Let $M = 
 	\begin{pmatrix}
 		\kb & \jb \\
 		\ib & 1
 	\end{pmatrix}$.
 	Then, $Cdet(M) = \kb-\ib\jb = \kb-\kb = 0$. Hence, we can say that by the Cayley determinant, $M$ is singular.
 \end{ex}
 \begin{ex}
 	\cite{aslaksen} Consider the transpose of the matrix $M$, $M^T = 
 	\begin{pmatrix} \label{invertbutnot}
 		\kb & \ib \\
 		\jb & 1
 	\end{pmatrix}$.
 	Then $Cdet(M^T) = \kb-(\jb\ib) = \kb - (-\kb) = 2\kb$. Hence, we can say that by the Cayley determinant, $M^T$ is invertible.
 \end{ex}
  Taking into account the fact that the quaternions are non-commutative, one might ask whether or not this determinant behaves the way we expect - Will it really determine whether or not a quaternionic matrix is singular or not? Will the properties of the determinant still hold? Will the determinant still be a map from $M_{n}(G) \rightarrow G$ (in this case, $G = \HH$) where $M_{n}(G)$ is the set of all $n\times n$ matrices over the elements of $G$? The last question comes from the fact that the determinant of a complex matrix yields a complex number.
}


\subsection{Brenner's Determinant Function and Aslaksen's Axioms}

We take a step back and revisit what it means for a mapping to be 	a determinant. J.L. Brenner \cite{brenner} and Helmer Aslaksen \cite{aslaksen} offer different approaches to this, however, we will see how we can both arrive at the same conclusions. The following definition is taken from \cite{brenner}.

\begin{definition}[Determinant Function] \label{detfn}
	\emph{\cite{brenner}} For a field $F$, a determinant over the matrices of $\Mf{n}$ is a function $det$ from $\Mf{n}$ into $F$ such that 
	\begin{equation}
	det(AB) = det(A)det(B) = det(B)det(A)
	\end{equation} 
	holds either
	\begin{enumerate}
		\item $\forall A, B \in \Mf{n}$ or
		\item $\forall$ invertible $A, B \in \Mf{n}$
	\end{enumerate}. 
\end{definition} 

Aslaksen, on the other hand, uses a more axiomatic approach in defining a determinant, presenting three determinant \emph{axioms} which a determinant definition must satisfy in order for it to behave the way we expect, i.e., it has the properties we associate with determinants. 
\begin{itemize}
	\item \textbf{Axiom 1.} $det(A) = 0$ if and only if $A$ is singular.
	\item \textbf{Axiom 2.} $det(AB) = det(A)det(B)$ for all quaternionic matrices $A$ and $B$.
	\item \textbf{Axiom 3.} If $A'$ is obtained by adding a left-multiple of a row to another row or a right-multiple of a column to another column, then $det(A')=det(A)$ (as we have already encountered in linear algebra, this operation can be described by an elementary matrix \cite{aslaksen}).
\end{itemize}

Notice that a determinant is essentially a function that:
\begin{enumerate}
 \item Maps to 0 if a matrix is singular
 \item Preserves multiplication and,
 \item Remains unchanged after applying the elementary operation of adding a left/right-multiple of one row/column to another row/column respectively.
\end{enumerate}

Also notice that Alaksen's second axiom is the first condition in Brenner's determinant function.

We can define the determinant simply as a function that constantly maps to 0 for all singular matrices and 1 for all non-singular matrices \cite{brenner}. This will satisfy the above axioms \cite{brenner} \cite{aslaksen}, however, we will mostly deal with determinants that are non-trivial (for instance the $\rrdet$, $\ccdet$, and $Cdet$). 

\begin{theorem} \label{nontrivdetfn}
\emph{\cite{brenner}} If $det$ is not constantly equal to 1 or 0 (i.e., $det$ is not a mapping $det: \Mf{n} \rightarrow F$ where $F$ is a field with two elements), then $det(B) = 0$ for all singular matrices. 
\end{theorem}

Theorem \ref{nontrivdetfn} not only shows how the determinant function in \ref{detfn} holds for the non-trivial case, it also shows that conditions (1) and (2) of Definition \ref{detfn} are essentially equivalent \cite{brenner}. This means that a determinant should map all singular matrices to 0. 

\begin{theorem} \label{comh}
	\emph{\cite{aslaksen}} Let $\Mh{n}$ be the set of all $n\times n$ quaternionic matrices. If $det$ satisfies all of Aslaksen's axioms, then $det(\Mh{n})$ is a commutative subset of $\HH$. 
\end{theorem}

Notice how Theorem \ref{comh} is already implied in Brenner's determinant function in which it is already deemed necessary for the images to commute. By Theorem \ref{comh}, we see that $det$ satisfying Aslaksen's axioms must only map to a commutative subset of $\HH$, which is the complex numbers. Therefore, $det$ cannot be a mapping onto $\HH$. Since $Cdet$ is onto $\HH$, by contrapositive of theorem \ref{comh}, $Cdet$ does not satisfy at least one of the axioms \cite{aslaksen}. 

\begin{ex}
\cite{aslaksen} As an illustration, we show that $Cdet$ doesn't satisfy Axiom 1. Recall examples \ref{singbutnot} and \ref{invertbutnot}. Notice that,
\begin{align*}
	&\text{\indent\indent\indent}M\begin{pmatrix}
	x \\
	y
	\end{pmatrix} = 
	\begin{pmatrix}
	0 \\
	0
	\end{pmatrix} \\ &\implies
	\begin{pmatrix}
	 		\kb & \jb \\
	 		\ib & 1
	\end{pmatrix}
	\begin{pmatrix}
	x \\
	y
	\end{pmatrix} = 
	\begin{pmatrix}
	0 \\
	0
	\end{pmatrix} \\ &\implies
	\kb x + \jb y = 0 \text{ and } \ib x + y = 0 \\ &\implies
	x = 0 \text{ and } y = 0. \\ &\implies
	M \text{ is invertible. This contradicts with the fact that $Cdet(M) = 0$.}
\end{align*}

Also notice that,

\begin{align*}
	&M^T\begin{pmatrix}
	-1 \\
	\jb
	\end{pmatrix} = 
	\begin{pmatrix}
	 		\kb & \ib \\
	 		\jb & 1
	\end{pmatrix}
	\begin{pmatrix}
	-1 \\
	\jb
	\end{pmatrix} = 
	\begin{pmatrix}
	0 \\
	0
	\end{pmatrix} \\ &\implies
	M^T \text{ is singular. This contradicts with the fact that $Cdet(M^T) = 2\kb$.}
\end{align*}
\end{ex}
 In \cite{aslaksen}, it is also shown that $Cdet$ doesn't satisfy Axioms 2 and 3.

\section{Matrix Homomorphisms}

Because the Cayley determinant fails to satisfy Aslaksen's axioms, the classical definition of the determinant cannot be extended to quaternionic matrices. It was not until 1920, that a new approach in defining a quaternionic determinant was presented in \cite{aslaksen}. The idea is to transform quaternionic matrices into complex matrices from which one could then just simply take the determinant \cite{aslaksen}. The method involves homomorphisms between quaternionic, complex, and real matrices. 

 In this section, we take a closer look into these homomorphisms - first discussing the motivation behind them and then the theory. 

\subsection{Representing Complex Numbers as Real Matrices}

Recall in Chapter 2 that $\CC$ forms a complex vector space. For us to represent a complex number $a+b\ib$ as a real matrix, we first have to express it as a linear map $f: \CC \rightarrow \CC$, i.e., a $1\times 1$ complex matrix $[a+b\ib]$. Thus, we have $f(z) = [a+b\ib]z$ which is identical to complex multiplication by $a+b\ib$. We see that the images of $1$ and $\ib$ under $f$ are $a+b\ib$ and $-b+a\ib$ respectively. We know from abstract algebra that we can define a bijection from the field of complex numbers to the 2D-plane ($\R^2$) - a mapping $\Theta : \CC \rightarrow \R^2$ where a complex number $a+b\ib$ is mapped to a vector/point $(a,b)$ in the 2D-plane. Under the function $\Theta$ (in which case $1$ is mapped to $(1,0)$ while $\ib$ is mapped to $(0,1)$), we seek a $2 \times 2$ real matrix that maps $(1,0)$ to $\Theta(a+b\ib) = (a,b)$ and $(0,1)$ to $\Theta(-b+a\ib) = (-b,a)$. 
\\
\noindent Let this matrix be $F =$ 
\begin{pmatrix} 
\alpha & \beta \\ 
\chi & \delta 
\end{pmatrix} where $\alpha, \beta, \chi, \text{ and } \delta \in \R$. Then, 
\begin{align*}
	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		1 \\ 0 
	\end{pmatrix} &= 
	\begin{pmatrix} a \\ b \end{pmatrix} \implies
	\begin{pmatrix}
		\alpha \\ \chi
	\end{pmatrix} =
	\begin{pmatrix} a \\ b \end{pmatrix} \implies 
	\alpha = a; \chi = b \text{ and} \\
	\begin{pmatrix} 
		\alpha & \beta \\ 
		\chi & \delta 
	\end{pmatrix} 
	\begin{pmatrix} 
		0 \\ 1 
	\end{pmatrix} &= 
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\begin{pmatrix}
		\beta \\ \delta
	\end{pmatrix} =
	\begin{pmatrix} -b \\ a \end{pmatrix} \implies
	\beta = -b; \delta = a
\end{align*}
\noindent Therefore, $F = \begin{pmatrix} \alpha & \beta \\ \chi & \delta \end{pmatrix} = \begin{pmatrix} \label{phismall} a & -b \\ b & a \end{pmatrix}$. 

\noindent\textit{Remark.} The matrix $F$ commutes with the complex structure in $\R^2$, $J = 
\begin{pmatrix}
 		0 & -1 \\
 		1 & 0
 \end{pmatrix}$, since, 
 \begin{align*}
	\begin{pmatrix}
	 	a & -b \\
	 	b & a
	 \end{pmatrix}
	\begin{pmatrix}
	 		0 & -1 \\
	 		1 & 0
	 \end{pmatrix} = 
	 \begin{pmatrix}
	 	-b & -a \\
	 	a & -b
	 \end{pmatrix} =
	 \begin{pmatrix}
	 		0 & -1 \\
	 		1 & 0
	 \end{pmatrix}
	 \begin{pmatrix}
	 	a & -b \\
	 	b & a
	 \end{pmatrix}
\end{align*}
Therefore, the matrix $F$ is a real linear map that represents the complex linear map $f$.

\begin{ex}
	Take the complex number $z = -3+2\ib$. Then its real matrix representation is
	$\begin{pmatrix}
		-3 & -2 \\
		2 & -3
	\end{pmatrix}$. 
	We can use this real matrix representation to multiply $z$ with another complex number, say $1+2\ib$. 
	$\begin{pmatrix}
		-3 & -2 \\
		2 & -3
	\end{pmatrix}
	\begin{pmatrix}
		1 \\ 2
	\end{pmatrix}
	 = 
	\begin{pmatrix}
		-7 \\
		-4
	\end{pmatrix}$
	which corresponds to the complex number obtained by multiplying $z(1+2\ib) = (1+2\ib)z = -7-4\ib$.
\end{ex}

It is important to note that $F$ in general, doesn't represent the complex number $a+b\ib$ itself but the linear map associated with multiplying a complex number by $a+b\ib$. In this case, $F$ can represent both left and right multiplication because complex numbers are commutative under multiplication. 

In representing the complex number $a+b\ib$ as a real matrix, we associate it with the real matrix $F$, i.e., we define a map $\phi : \CC \rightarrow \Mr{2}$ where $\Mr{2}$ is the set of all $2 \times 2$ real matrices, such that $a+b\ib \mapsto 
\begin{pmatrix}
	a & -b \\
	b & a
\end{pmatrix}$

In order for a real matrix representation to represent one and only one complex number $a+b\ib$, $\phi$ must be \emph{injective}. $\phi$ must also preserve the structure that complex multiplication gives to $\CC$, i.e., $\phi$ must also be a homomorphism. We've already shown in Chapter 2 Example \ref{ex:homomorph} that $\phi$ is indeed a homomorphism. In the next subsection, we show that $\phi$ is, in fact, an \emph{injective homomorphism} in the general case.

\subsection{Homomorphisms from $\Mc{n}$ to $\Mr{2n}$}

Let $\Mc{n}$ denote the set of all $n\times n$ complex matrices and $\Mr{2n}$ denote the set of all $2n \times 2n$ real matrices. When we represent complex matrices as real matrices, we are essentially representing complex linear maps as real linear maps. Recall in Chapter 2, that in order for a real linear map to represent a complex linear map, the said real map must commute with the complex structure defined in the corresponding real space $\R^{2n}$. 
We can define a matrix 
\begin{equation*} 
J = 
\begin{pmatrix} 
0 & -I_n \\ 
I_n & 0 
\end{pmatrix} 
\end{equation*}
We see that,
 \begin{align*}
 	J^2 = 
 	\begin{pmatrix}
 		0 & -I_n \\
 		I_n & 0
 	\end{pmatrix}
 	\begin{pmatrix}
 		0 & -I_n \\
 		I_n & 0
 	\end{pmatrix}
 	= 
 	\begin{pmatrix}
 		-I_n & 0 \\
 		0 & -I_n
 	\end{pmatrix}
 	= -I_{2n}
 \end{align*}
  Therefore, by Definition \ref{def:compstruct} $J$ gives a \emph{complex structure} in $\R^{2n}$. Let $\phi: \Mc{n} \rightarrow \Mr{2n}$ such that the image of a complex matrix under $\phi$ is its real matrix representation. Then $\phi(\Mc{n}) = \{P \in \Mr{2n} | JP = PJ\}$, i.e., the real matrix representations of complex matrices are those $2n \times 2n$ real matrices that commute with $J$ \cite{aslaksen}.   

The question now is which $2n \times 2n$ real matrices matrices commute with the complex structure in $\R^2$. Notice that every complex matrix can be represented as the sum of a real matrix and a purely imaginary matrix, i.e., for an $n\times n$ complex matrix $Z$, $Z = A + B\ib$ where $A,B \in \Mr{n}$ \cite{aslaksen}. We define a mapping 
\begin{equation} 
\label{eq:phimap}
	\phi(A+B\ib) = 
	\begin{pmatrix} 
	A & -B \\ B & A 
	\end{pmatrix}  
\end{equation} 

Notice that $F$ in \ref{phismall} is a special case of \ref{eq:phimap} where $n = 1$. If we can show that $\phi$ is an injective homomorphism (i.e., it preserves multiplication and is one-to-one), we can essentially represent any complex matrix as a $2n \times 2n$ real matrix. Also notice how the dimension of the real matrix is necessarily even. This is a direct consequence of Theorem \ref{dnc}.

\begin{theorem} \label{phimorph}
	Let $\phi: \Mc{n} \rightarrow \Mr{2n}$ such that $C+D\ib \mapsto $ \begin{pmatrix} C & -D \\ D & C \end{pmatrix} where $C+D\ib \in \Mc{n}$. Then $\phi$ is an injective homomorphism. 
\end{theorem}

\begin{proof}
	First, we show that $\phi$ is injective. Let $A+B\ib, C+D\ib \in \Mc{n}$. Then,
	\begin{align*}
		&\phi(A+B\ib) = \phi(C+D\ib) \\
		&\implies 
		\begin{pmatrix}
		A & -B \\ 
		B & A 
		\end{pmatrix} = 
		\begin{pmatrix}
		C & -D \\ 
		D & C 
		\end{pmatrix} \\
		&\implies A = C \text{ and } B = D \text{ by Matrix Equality} \\
		&\implies A+B\ib = C+D\ib \\
		&\implies \phi \text{ is injective.}
	\end{align*}
	
	We now show that $\phi$ is a homomorphism. Let $A+B\ib$, $C+D\ib \in \Mc{n}$. Then 
	\begin{align*}
		\phi[(A+B\ib)(C+D\ib)] &= \phi[(A+B\ib)C+(A+B\ib)D\ib] \text{ by Theorem \ref{distributive}} \\
		&= \phi[AC+BC\ib+AD\ib-BD] \\
		&= \phi[(AC-BD)+(BC+AD)\ib] \\
		&= 
		\begin{pmatrix} 
		AC-BD & -(BC+AD) \\ 
		BC+AD & AC-BD 
		\end{pmatrix}
	\end{align*}
	\begin{align*}
		\phi[(A+B\ib)]\phi[(C+D\ib)] &= 
		\begin{pmatrix}
		A & -B \\ 
		B & A 
		\end{pmatrix}
		\begin{pmatrix}
		C & -D \\ 
		D & C 
		\end{pmatrix} \\
		&=
		\begin{pmatrix} 
		AC-BD & -(BC+AD) \\ 
		BC+AD & AC-BD
		\end{pmatrix} \text{ by Theorem \ref{bigmatm}.} \\
		\text{Therefore, }\phi[(A+B\ib)(C+D\ib)] &= \phi[(A+B\ib)]\phi[(C+D\ib)] \text{ implying that $\phi$ is a homomorphism.}
	\end{align*}
Thus, $\phi$ is an injective homomorphism.
\end{proof}
\newline

 \begin{ex}
 	Take the complex matrix 

 	$Z = $
 	\begin{pmatrix}
 		1+2\ib & 3\ib \\
 		1 & -2+\ib
 	\end{pmatrix} $ = $
 	\begin{pmatrix}
 		1 & 0 \\
 		1 & -2
 	\end{pmatrix} $+$
 	\begin{pmatrix}
 		2 & 3 \\
 		0 & 1
 	\end{pmatrix} $\ib$.
 	Then, $\phi(Z) = $
 	\begin{pmatrix}
 		\begin{matrix}
 		1 & 0 \\
 		1 & -2
 		\end{matrix} & 
 		\begin{matrix}
 		-2 & -3 \\
 		1 & -1
 		\end{matrix} \\
 		\begin{matrix}
 		2 & 3 \\
 		0 & 1
 		\end{matrix} &
 		\begin{matrix}
 		1 & 0 \\
 		1 & -2
 		\end{matrix}
 	\end{pmatrix}
We know that $\phi(Z)$ is the only real matrix representation of $Z$ and that $\phi(Z)$ corresponds to the complex map $Z$ in the complex vector space $\CC^2$ since $\phi$ is an injective homomorphism.
 \end{ex}

\subsection{Representing Quaternions as Complex Matrices} \label{qrep}

Recall in Chapter 2 that we can view the quaternions as a 2-dimensional algebra over $\CC$ by having $\quat{a}{b}{c}{d} = (a+b\ib)+\jb(c-d\ib)$. In general, we can write any quaternion as $x+\jb y$ where $x, y \in \CC$. Because of this, we see that we can define a bijection $\Omega: \HH \rightarrow \CC^2$ where a quaternion $q = x+\jb y$ is mapped to $(x, y) \in \CC^2$. In representing a quaternion $q = x+\jb y$ as a complex matrix, we first have to express it as a linear map $s: \HH \rightarrow \HH$, i.e., a $1 \times 1$ quaternionic matrix $[x+\jb y]$. Thus, we have $s(q) = [x+\jb y]q$ which is identical to left quaternionic multiplication by $x+\jb y$. We see that,
\begin{align*}
	&s(1) = x+\jb y \\
	&s(\ib) = (x+\jb y)\ib = x\ib + \jb (y\ib) \\
	&s(\jb) = (x+\jb y)\jb = x\jb + \jb y\jb = \jb\bar{x}+\jb^2 \bar{y} = -\bar{y}+\jb\bar{x} \\
	&s(\kb) = (x+\jb y)\kb = x\kb + \jb y\kb = -x\jb\ib + \bar{y}\jb\kb = \bar{y}\ib - \jb(\bar{x}\ib) 
\end{align*}

\newcommand{\kmat}{\begin{pmatrix} \kappa & \lambda \\ \mu & \nu \end{pmatrix}}
\newcommand{\krmat}{\begin{pmatrix} \kappa_R & \lambda_R \\ \mu_R & \nu_R \end{pmatrix}}


Under the function $\Omega$, we see that the images of 1, $\ib, \jb$ and $\kb$ are $(1,0) , (\ib,0), (0,1)$ and $(0,-\ib)$ respectively and the images of $s(1), s(\ib), s(\jb)$ and $s(\kb)$ are $(x,y), (x\ib,y\ib), (-\bar{y},\bar{x})$ and $(\bar{y}\ib,-\bar{x}\ib)$. We seek a matrix in $\Mc{2}$ such that $(1,0) \mapsto (x,y), (\ib,0) \mapsto (x\ib,y\ib), (0,1) \mapsto (-\bar{y},\bar{x})$, and $(0,-\ib) \mapsto (\bar{y}\ib,-\bar{x}\ib)$.

 Let this matrix be $S = \kmat$ , where $\kappa, \lambda, \mu, $ and $\nu \in \CC$. Then, 

\begin{align*}
	&\kmat \vectC{1}{0} = \vectC{x}{y} \implies \vectC{\kappa}{\mu} = \vectC{x}{y} \implies \kappa = x \text{ and } \mu = y \\
	&\kmat \vectC{\ib}{0} = \vectC{x\ib}{y\ib} \implies \vectC{\kappa\ib}{\mu\ib} = \vectC{x\ib}{y\ib} \implies \kappa = x \text{ and } \mu = y \\
	&\kmat \vectC{0}{1} = \vectC{-\bar{y}}{\bar{x}} \implies \vectC{\lambda}{\nu} = \vectC{-\bar{y}}{\bar{x}} \implies \lambda = -\bar{y} \text{ and } \nu = \bar{x} \\
	&\kmat \vectC{0}{-\ib} = \vectC{\bar{y}\ib}{-\bar{x}\ib} \implies \vectC{-\lambda\ib}{-\nu\ib} = \vectC{\bar{y}\ib}{-\bar{x}\ib} \implies \lambda = -\bar{y} \text{ and } \nu = \bar{x}
\end{align*}

Hence, $S = \kmat = $
\begin{pmatrix}
	x & -\bar{y} \\
	y & \bar{x}
\end{pmatrix}.
\newline
\newline
\noindent\textit{Remark.} The matrix $S$ commutes with the quaternionic structure $\Psi$ in Chapter 2 Example \ref{ex:quatstruct} since for a complex vector $v = \vectC{x}{y} \in \CC^2$,
\begin{align*}
	&\begin{pmatrix}
		x & -\bar{y} \\
		y & \bar{x}
	\end{pmatrix}
	\overline{
		\begin{pmatrix}
		0 & -1 \\
		1 & 0
	\end{pmatrix}
	\vectC{x}{y}
	} = 
	\begin{pmatrix}
		x & -\bar{y} \\
		y & \bar{x}
	\end{pmatrix}
	\vectC{-\bar{y}}{\bar{x}} = 
	\vectC{-x\bar{y}-\bar{y}\bar{x}}{-y\bar{y}+\bar{x}^2} \text{ and } \\
	&\overline{
		\begin{pmatrix}
			0 & -1 \\
			1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			x & -\bar{y} \\
			y & \bar{x}
		\end{pmatrix}
		\vectC{x}{y}
	} = 
	\overline{
		\begin{pmatrix}
			0 & -1 \\
			1 & 0
		\end{pmatrix}
		\vectC{x^2-\bar{y}y}{yx+\bar{x}y}
	} = \vectC{-x\bar{y}-\bar{y}\bar{x}}{-y\bar{y}+\bar{x}^2}
\end{align*}
Hence, the matrix $S$ is a complex linear map that represents the quaternionic linear map $s$.

\begin{ex}
	Take the quaternion $q = -2+\ib-5\jb+2\kb = -2+\ib + \jb(-5-2\ib)$. Then its complex matrix representation is 
	\begin{pmatrix}
		-2+\ib & 5-2\ib \\
		-5-2\ib & -2-\ib
	\end{pmatrix}.
	We can use this matrix to multiply $q$ with another quaternion, say $1+\ib+3\jb-4\kb = 1+\ib + \jb(3+4\ib)$. We have, 
	\begin{pmatrix}
		-2+\ib & 5-2\ib \\
		-5-2\ib & -2-\ib
	\end{pmatrix}
	$\vectC{1+\ib}{3+4\ib} = \vectC{20+13\ib}{-5-18\ib}$ which corresponds to the quaternion obtained by multiplying $q(1+\ib+3\jb-4\kb) = 20+13\ib-5\jb+18\kb = 20+13\ib+\jb(-5-18\ib)$ on the left.
\end{ex}
Notice that $S$ only represents left multiplication in the quaternions. One would probably ask whether or not we can have a complex matrix representation for right multiplication.
\newline
Consider the quaternionic function defined by multiplying a quaternion $q$ by some quaternion $x+\jb y$ on the right, $s_R(q) = q(x+\jb y)$ such that $y\neq 0$. We see that,
\begin{align*}
	s_R(1) &= x+\jb y \\
	s_R(\ib) &= \ib x - \jb\ib y \\
	s_R(\jb) &= -y + \jb x \\
	s_R(\kb) &= -\ib y -\jb\ib x 
\end{align*}

We use the same method in obtaining the complex matrix representation. We let $S_R = $ \krmat be the complex matrix representation of this function. 

\begin{align*}
	&\krmat \vectC{1}{0} = \vectC{x}{y} \implies \vectC{\kappa_R}{\mu_R} = \vectC{x}{y} \implies \kappa_R = x \text{ and } \mu_R = y \\
	&\krmat \vectC{\ib}{0} = \vectC{\ib x}{-\ib y} \implies \vectC{\kappa_R\ib}{\mu_R\ib} = \vectC{\ib x}{-\ib y} \implies \kappa_R = x \text{ and } \mu_R = -y \\ 
	&\krmat \vectC{0}{1} = \vectC{-y}{x} \implies \vectC{\lambda_R}{\nu_R} = \vectC{-y}{x} \implies \lambda_R = -y \text{ and } \nu_R = x \\
	&\krmat \vectC{0}{-\ib} = \vectC{-\ib y}{-\ib x} \implies \vectC{-\lambda_R\ib}{-\nu_R\ib} = \vectC{-\ib y}{-\ib x} \implies \lambda_R = y \text{ and } \nu_R = x 
\end{align*}

This implies that $\mu_R = \lambda_R = 0$. Hence, the complex matrix representation of right multiplication by a quaternion is 
\begin{pmatrix}
	x & 0 \\
	0 & x
\end{pmatrix}.
However, we see that, 
\begin{pmatrix}
	x & 0 \\
	0 & x
\end{pmatrix}
\vectC{1}{0} $ = $ \vectC{x}{0} which doesn't correspond to $s_R(1) &= x+\jb y$. Thus, we see that there is no matrix representation for right multiplication \cite{aslaksen}. This is because right scalar multiplication itself, if expressed as a mapping $L_R$ such that $L_R(v) = vq$ for $q \in \HH$, does not satisfy Property 2 of Definition \ref{linmapdef} between right vector spaces, i.e., $L_R(v)c = vrc \neq L_R(vc) = vcr$. In representing a quaternion $x+\jb y$ as a complex matrix, we associate it with the complex matrix $S$, i.e., we define a map $\psi: \HH \rightarrow \Mc{2}$ where $\Mc{2}$ is the set of all $2 \times 2$ complex matrices, such that $x+\jb y \mapsto
		\begin{pmatrix}
			x & -\bar{y} \\
			y & \bar{x}
		\end{pmatrix}$.

In order for a complex matrix representation to represent one and only one quaternion $x+\jb y$, $\psi$ must be injective. $\psi$ must also preserve the structure that right quaternion multiplication gives to $\HH$, i.e., $\psi$ must be a homomorphism. In the next subsection, we show that $\psi$ is indeed an injective homomorphism.

\subsection{Homomorphisms from $\Mh{n}$ to $\Mc{2n}$}

Let $\Mh{n}$ be the set of all $n\times n$ quaternionic matrices and $\Mc{2n}$ be the set of all $2n \times 2n$ complex matrices. When we represent quaternionic matrices as complex matrices, we are essentially representing quaternionic linear maps as complex linear maps. Recall in Chapter 2, that in order for a complex linear map to represent a quaternionic linear map, the said complex map must commute with the quaternionic structure defined in the corresponding complex space $\CC^{2n}$. 

For $v = \vectC{\chi}{\gamma} \in \CC^{2n}$ where $\chi = 
\begin{pmatrix}
	x_1 \\
	\vdots \\
	x_n
\end{pmatrix}$
 and $\gamma = 
 \begin{pmatrix}
	y_1 \\
	\vdots \\
	y_n
\end{pmatrix}$, we can define a mapping,
\begin{align*}
	\Psi(v) = 
	\overline{
	\begin{pmatrix}
		0 & -I_n \\
		I_n & 0
	\end{pmatrix}\vectC{\chi}{\gamma}}
	= \vectC{-\overline{\gamma}}{\overline{\chi}}
\end{align*}
We see that $\Psi$ satisfies Definition \ref{def:quatstruct}, i.e., $\Psi$ gives a \emph{quaternionic structure} in $\CC^{2n}$ since, 
\begin{align*}
	\Psi^2(v) = \Psi(\Psi(v)) = 
	\overline{
	\begin{pmatrix}
		0 & -I_n \\
		I_n & 0
	\end{pmatrix}\vectC{-\overline{\gamma}}{\overline{\chi}}}
	= \vectC{-\chi}{-\gamma} = -v.
\end{align*}
Let $\psi: \Mh{n} \rightarrow \Mc{2n}$ such that the image of a quaternionic under $\psi$ is its complex matrix representation. Then the complex matrix representations of quaternionic matrices are those $2n \times 2n$ complex matrices $N$ that commute with $\Psi$, i.e., 
\begin{equation} 
	N\overline{Jv} = \overline{JNv}.
	\label{eq:bigquatstruct}
\end{equation}
We can simplify Equation \ref{eq:bigquatstruct}. We have $N\overline{Jv} = \overline{JNv}$ if and only if $\overline{\overline{N}Jv} = \overline{JNv}$ which means that $\overline{N}J = JN$ or $NJ = J\overline{N}$. Notice that Theorem \ref{jx} in Chapter 2 (i.e. for $z \in \CC$, $z\jb = \jb\bar{z}$) is a special case of $NJ = J\overline{N}$ where $n = 1$. Hence, the complex matrices that commute with the quaternionic structure are those $2n \times 2n$ complex matrices $N$, such that $NJ = J\overline{N}$, i.e., $\psi(\Mh{n}) = \{N \in \Mc{2n} | NJ = J\overline{N}\}$. 

The question now is which $2n \times 2n$ complex matrices $N$ satisfy $NJ = J\overline{N}$. Notice that every quaternionic matrix can be represented as the sum $Q = X + \jb Y$ where $X, Y \in \Mc{n}$ \cite{aslaksen}. We define a mapping 
	\begin{equation} 
	\psi(X+\jb Y) = 
	\begin{pmatrix} 
	X & -\overline{Y} \\ 
	Y & \overline{X} 
	\end{pmatrix}  
	\label{eq:psimap}
\end{equation} 

Notice that the matrix $S$ is a special case of \ref{eq:psimap}. Again, if we can show that $\psi$ is an injective homomorphism, we can essentially represent any quaternionic matrix as a $2n \times 2n$ complex matrix.

\begin{theorem} \label{psimorph}
 	Let $\psi: \Mh{n} \rightarrow \Mc{2n}$ such that $X+\jb Y \mapsto $ 
 	\begin{pmatrix} 
 	X & -\overline{Y} \\ 
 	Y & \overline{X} 
 	\end{pmatrix} 
 	where $X+\jb Y \in \Mh{n}$. Then $\psi$ is an injective homomorphism. 
\end{theorem}

\begin{proof}
	We first show that $\psi$ is injective. Let $X+\jb Y, V+\jb W \in \Mh{n}$. Then,
	\begin{align*}
		&\psi(X+\jb Y) = \psi(V+\jb W) \\
		&\implies 
		\begin{pmatrix}
		X & -\overline{Y} \\ 
 		Y & \overline{X} 		
 		\end{pmatrix} = 
		\begin{pmatrix}
		V & -\overline{W} \\ 
 		W & \overline{V} 
		\end{pmatrix} \\
		&\implies X = V \text{ and } Y = W \text{ by Matrix Equality} \\
		&\implies X+\jb Y = V+\jb W \\
		&\implies \psi \text{ is injective.}
	\end{align*}
	We now show that $\psi$ is a homomorphism. Let $X+\jb Y$, $V+\jb W \in \Mh{n}$. Then 
	\begin{align*}
		\psi[(X+\jb Y)(V+\jb W)] &= \psi[X(V+\jb W)+\jb Y(V+\jb W)] \text{ by Theorem \ref{distributive}} \\
		&= \psi[XV+X\jb W + \jb YV+ \jb Y\jb W] \\
		&= \psi[XV+\jb \overline{X}W + \jb YV + \jb^2\overline{Y}W] \\
		&= \psi[(XV-\overline{Y}W)+\jb(\overline{X}W+YV)] \\
		&=
		\begin{pmatrix} 
		XV-\overline{Y}W & -\overline{(\overline{X}W+YV)} \\ 
		\overline{X}W+YV & \overline{XV-\overline{Y}W} 
		\end{pmatrix} \\
		&=
		\begin{pmatrix} 
		XV-\overline{Y}W & -X\overline{W}-\overline{Y}\overline{V} \\ 
		\overline{X}W+YV & \overline{X}\overline{V}-Y\overline{W} 
		\end{pmatrix}
		\end{align*}
		
		\begin{align*}
		\psi[(X+\jb Y)]\psi[(V+\jb W)] &= 
		\begin{pmatrix}
		X & -\overline{Y} \\ 
		Y & \overline{X} 
		\end{pmatrix}
		\begin{pmatrix}
		V & -\overline{W} \\ 
		W & \overline{V} 
		\end{pmatrix} \\
		&=
		\begin{pmatrix} 
		XV-\overline{Y}W & -X\overline{W}-\overline{Y}\overline{V} \\ 
		\overline{X}W+YV & \overline{X}\overline{V}-Y\overline{W} 
		\end{pmatrix} \text{ by Theorem \ref{bigmatm}.}
	\end{align*}
\end{proof}


\begin{ex}
	Take the quaternionic matrix 

	$Q = 
	\begin{pmatrix}
		1+2\ib-3\jb+\kb & 2\ib+5\kb \\
		1-\ib & 3+\jb+\kb
	\end{pmatrix}  = 
	\begin{pmatrix}
		1+2\ib & 2\ib \\
		1-\ib & 3
	\end{pmatrix}+\jb
	\begin{pmatrix}
		-3-\ib & -5\ib \\
		0 & 1-\ib
	\end{pmatrix}$.

	Hence, $\psi(Q) = 
	\begin{pmatrix}
		\begin{matrix}
			1+2\ib & 2\ib \\
			1-\ib & 3
		\end{matrix} &
		\begin{matrix}
			3-\ib & -5\ib \\
			0 & -1-\ib
		\end{matrix} \\
		\begin{matrix}
			-3-\ib & -5\ib \\
			0 & 1-\ib
		\end{matrix} &
		\begin{matrix}
			1-2\ib & -2\ib \\
			1+\ib & 3
		\end{matrix} 
	\end{pmatrix}$.
We know that $\psi(Q)$ is the only complex matrix representation of $Q$ and that $\psi(Q)$ corresponds to the quaternionic map $Q$ in the right quaternionic vector space $\HH^2$ since $\psi$ is an injective homomorphism.
\end{ex}
\newpage
\section{The Study Determinant}

The \emph{Study Determinant} uses the homomorphisms $\phi$ and $\psi$ to transform quaternionic matrices into complex or real matrices. We can then compute for the determinant of the complex or real matrix obtained by applying $\phi$ and $\psi$.

\begin{definition}[Study Determinant]
	\emph{\cite{aslaksen}} For $M \in \Mh{n}$, the \emph{Study Determinant} is defined by $Sdet(M) = \cdet{\psi(M)} = \sqrt{\rdet{\phi(\psi(M))}}$.
\end{definition}

Notice that we can compute for the Study determinant in two different ways: 
\begin{enumerate}
	\item Simply getting the complex matrix representation of the quaternionic matrix and proceeding to take its complex determinant or;
	\item Getting the real matrix representation of the quaternionic matrix by composing $\phi$ and $\psi$, and then taking the square root of the real determinant.
\end{enumerate}

In the following example, we take the Study determinant of the quaternionic matrix in Example \ref{singbutnot} for which the Cayley determinant failed to identify as non-singular. 

\begin{ex}
	Let $M = $ 
	\begin{pmatrix}
 		\kb & \jb \\
 		\ib & 1
 	\end{pmatrix} $ = $
 	\begin{pmatrix}
 		0 & 0 \\
 		\ib & 1
 	\end{pmatrix} $+\jb$
 	\begin{pmatrix}
 		-\ib & 1 \\
 		0 & 0
 	\end{pmatrix}.
 	Then,

 	\begin{enumerate}
 		\item $Sdet(M) = \ccdet(\psi(M)) = \ccdet
		\begin{pmatrix}
			\begin{matrix}
				0 & 0 \\
				\ib & 1
			\end{matrix} &
			\begin{matrix}
				-\ib & -1 \\
				0 & 0
			\end{matrix} \\
			\begin{matrix}
 				-\ib & 1 \\
 				0 & 0
 			\end{matrix} &
 			\begin{matrix}
				0 & 0 \\
				-\ib & 1
			\end{matrix}
 		\end{pmatrix}  = 4$. 
 		\item We see that $\psi(M) = 
 		\begin{pmatrix}
 			0 & 0 & 0 & -1 \\
 			0 & 1 & 0 & 0 \\
 			0 & 1 & 0 & 0 \\
 			0 & 0 & 0 & 1
 		\end{pmatrix} +
 		\begin{pmatrix}
 			0 & 0 & -1 & 0 \\
 			1 & 0 & 0 & 0 \\
 			-1 & 0 & 0 & 0 \\
 			0 & 0 & -1 & 0
 		\end{pmatrix} \ib$.

 		$\sqrt{\rrdet(\phi(\psi(M)))} =$
 		$\begin{pmatrix}
 		\rrdet
 		\begin{pmatrix}
 			0 & 0 & 0  & -1 &  0 & 0 & 1 &  0 \\
 			0 & 1 & 0  &  0 & -1 & 0 & 0 &  0 \\
 			0 & 1 & 0  &  0 &  1 & 0 & 0 &  0 \\
 			0 & 0 & 0  &  1 &  0 & 0 & 1 &  0 \\
 			0 & 0 & -1 &  0 &  0 & 0 & 0 & -1 \\
 			1 & 0 & 0  &  0 &  0 & 1 & 0 &  0 \\
 		   -1 & 0 & 0  &  0 &  0 & 1 & 0 &  0 \\
 		    0 & 0 & -1 &  0 &  0 & 0 & 0 &  1 \\
 		\end{pmatrix}
 		\end{pmatrix}^{1/2} = \sqrt{16} = 4$
 	\end{enumerate}


\end{ex}

Hence, we can see that by the Study determinant, the matrix $M$ is invertible which it really is as shown in section \ref{cayley}. 

The Study Determinant holds for \textbf{Axiom 2} since $\psi$ and $\phi$ are homomorphisms \cite{aslaksen}.

As for \textbf{Axiom 1}, notice that for an $n\times n$ quaternionic matrix $M$, $\ccdet(\psi(M))$ is a complex determinant. When $\ccdet(\psi(M)) \neq 0 $ , the complex matrix $\psi(M)$ is invertible. We need to see if the inverse of $\psi(M)$ is also a complex matrix representation of a quaternionic linear map, i.e., $\psi(M)^{-1} \in \psi(\Mh{n}) = \{N \in \Mc{2n} | NJ = J\overline{N}\}$ \cite{aslaksen}. Since $\psi(M) \in \psi(\Mh{n})$, $\psi(M)J = J\overline{\psi(M)}$. By getting the inverse of both sides, we get 
\begin{align*}
	(\psi(M)J)^{-1} &= (J\overline{\psi(M)})^{-1}\\
	J^{-1}\psi(M)^{-1} &= \overline{\psi(M)}^{-1}J^{-1}\\
	J\psi(M)^{-1} &= \overline{\psi(M)^{-1}}J
\end{align*}
Hence, the inverse is in $\psi(\Mh{n})$. This implies that \textbf{Axiom 1} holds for the Study Determinant.

We can represent the elementary row operation described in \textbf{Axiom 3} using an $n \times n$ matrix \\$B_{ij}(b) = 
		\begin{pmatrix}
			1 & 0 & \cdots & 0 \\
			0 & 1 & \cdots & 0 \\
			0 & b & \cdots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \cdots & 1 
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 0 & 0 & \cdots & 0 \\
			0 & 1 & 0 & \cdots & 0 \\
			0 & 0 & 1 & \cdots & 0 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \cdots & 1
		\end{pmatrix} + b
		\begin{pmatrix}
			0 & 0 & 0 & \cdots & 0 \\
			0 & 0 & 0 & \cdots & 0 \\
			0 & 1 & 0 & \cdots & 0 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \cdots & 0
		\end{pmatrix} = I_n + be_{ij}$ \\where $e_{ij}$ is the $n\times n$ matrix with 1 on the $ij^{th}$ entry (for $i \neq j$) and 0 elsewhere \cite{aslaksen}. Notice that $e_{ij}e_{ij}=0$ since $i \neq j$.

		Multiplying by $B_{ij}(b)$ on the left adds the left scalar multiple by $b$ of the $j^{th}$ row to the $i^{th}$ row and multiplying by $B_{ij}(b)$ on the right adds the right scalar multiple by $b$ of the $i^{th}$ column to the $j^{th}$ column \cite{aslaksen}.

		Since by \textbf{Axiom 2}, $Sdet(B_{ij}(b)M) = Sdet(B_{ij}(b))Sdet(M)$, we need to show that $Sdet(B_{ij}(b)) = 1$ in order to show that $Sdet(M)$ doesn't change after applying $B_{ij}(b)$. Before we proceed, we present Theorem \ref{theorem:detmatgen} which allows us to compute for the determinant of $2m\times 2m$ matrices.

\begin{theorem} \label{theorem:detmatgen}
	\emph{\cite{aslaksen}} If $A_{11}, A_{12}$ and $A_{22}$ are $m \times m$ matrices that are mutually commutative, then,
	\begin{equation*}
		det
		\begin{pmatrix}
			A_{11} & A_{12} \\
			A_{21} & A_{22}
		\end{pmatrix}
		= det(A_{11}A_{22} - A_{12}A_{21})
	\end{equation*}
\end{theorem}

		If $b = x + \jb y$, then $B_{ij}(b) = I_n + xe_{ij} + \jb ye_{ij}$, hence,

		$\psi(B_{ij}(b)) = 
		\begin{pmatrix}
			I_n + xe_{ij} & -\bar{y}e_{ij} \\
			ye_{ij} & I_n+\bar{x}e_{ij}
		\end{pmatrix}$ and,
		\begin{align*}
			\ccdet(\psi(B_{ij}(b))) &= \ccdet[(I_n+xe_{ij})(I_n+\bar{x}e_{ij})+y\bar{y}e_{ij}e_{ij}] \\ &\text{ by Theorem \ref{theorem:detmatgen}.} \\
			&=\ccdet[(I_n+xe_{ij})(I_n+\bar{x}e_{ij})] \\
			&\text{since $e_{ij}e_{ij} = 0$.} \\
			&=\ccdet(I_n+xe_{ij})(\overline{I_n+xe_{ij}}) \\
			&=\ccdet(I_n+xe_{ij})\overline{(\ccdet(I_n+xe_{ij}))} \\
			&\text{by Theorem \ref{detbar}.} \\
			&=|\ccdet(I_n+xe_{ij})|^2 \\
			&=|\ccdet(I_n)|^2 \text{ since $B_{ij}(x)$ is a triangular matrix.} \\
			&= 1
		\end{align*}
		Hence, the Study Determinant satisfies \textbf{Axiom 3}.


\iffalse
\section{Skew-Coninvolutory Quaternionic Matrices of Odd Dimensions}
In Chapter 2, we saw in Theorem \ref{dnc} that there are no \emph{skew-coninvolutory complex matrices} of odd dimensions. In the real case, Theorem \ref{dnc} greatly influenced the dimensions of real vector spaces in which a \emph{complex structure} can be defined. We will show that Theorem \ref{dnc} also holds for quaternionic matrices, i.e., the set of all $n \times n$ skew-coninvolutory quaternionic matrices is empty when $n$ is odd.

\begin{theorem}[Main Result]
 Let $\mathscr{D}_n(\HH)$ denote the set of all skew-coninvolutory quaternionic matrices. Then $\mathscr{D}_n(\HH)$ is empty when $n$ is odd.
\end{theorem}
\begin{proof}
	If $F \in \mathscr{D}_{n}(\HH)$ then $F\bar{F} = -I_n$ by Definition \ref{skewquatmat} 
	\newline 
	We see that we can take the Study Determinant of both sides since $F$ is a quaternionic matrix, 
	$$Sdet(F\bar{F}) &= Sdet(-I_n)$$
	Notice that,
	\begin{align*}
		Sdet(-I_n) = \ccdet(\psi(-I_n)) = 
		\begin{pmatrix}
			\genmatk{-1}{0}{0}{-1}
		\end{pmatrix}
	\end{align*}
	\begin{align*}
		Sdet(F)Sdet(\bar{F}) &= (-1)^n \\
		Sdet(F)\overline{Sdet(F)} &= (-1)^n \text{, by Theorem \ref{detbar}} \\
		|Sdet(F)|^2 &= (-1)^n
	\end{align*}
	Since $|Sdet(F)|^2 > 0$, $(-1)^n > 0$. Hence, $n$ must be even.
\end{proof}

	Theorem \ref{detbar} applies because the Study determinant can be viewed as a complex determinant.

\begin{ex}
	Again consider the $1 \times 1$ case where, $\jb\bar{\jb} = -\jb^2 = 1 \neq -1$. Whereas in the $2 \times 2$ case, consider $E = $ 
	\begin{pmatrix}
		0 & \jb \\
		-\jb & 0
	\end{pmatrix}. Then, $E\overline{E} = $
	 \begin{pmatrix}
		0 & \jb \\
		-\jb & 0
	\end{pmatrix}
	\begin{pmatrix}
		0 & -\jb \\
		\jb & 0
	\end{pmatrix} $ = $
	\begin{pmatrix}
		-1 &  0 \\
		 0 & -1
	\end{pmatrix}.
\end{ex}
\fi